diff --git a/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.c b/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.c
index 98483f3..8b989d2 100644
--- a/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.c
+++ b/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.c
@@ -567,10 +567,25 @@ iris_bo_busy_syncobj(struct iris_bo *bo)
 }
 
 bool
-iris_bo_busy(struct iris_bo *bo)
+iris_bo_busy(struct iris_bo *bo, int id)
 {
    bool busy;
 
+
+   // iris_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
+	 if (id == 1) return false;
+	 // alloc_bo_from_cache
+	 if (id == 2) return false; /* can be false */
+	 // bo_free
+	 if (id == 3) return true; /* same as 4 */
+	 // cleanup_bo_cache
+	 if (id == 4) return false; /* small corruptions (black) */
+	 // resource_is_busy
+	 if (id == 5) return false; /* can be false */
+
+	 printf("%s:%d id: %d\n", __func__, __LINE__, id);
+	 return true;
+
    switch (iris_bufmgr_get_device_info(bo->bufmgr)->kmd_type) {
    case INTEL_KMD_TYPE_I915:
       if (iris_bo_is_external(bo))
@@ -676,7 +691,7 @@ iris_can_reclaim_slab(void *priv, struct pb_slab_entry *entry)
 {
    struct iris_bo *bo = container_of(entry, struct iris_bo, slab.entry);
 
-   return !iris_bo_busy(bo);
+   return !iris_bo_busy(bo, 1);
 }
 
 static void
@@ -1013,7 +1028,7 @@ alloc_bo_from_cache(struct iris_bufmgr *bufmgr,
        * either falling back to a non-matching memzone, or if that fails,
        * allocating a fresh buffer.
        */
-      if (iris_bo_busy(cur))
+      if (iris_bo_busy(cur, 2))
          return NULL;
 
       list_del(&cur->head);
@@ -1514,6 +1529,7 @@ bo_close(struct iris_bo *bo)
    free(bo);
 }
 
+static uint64_t zombie_total = 0;
 static void
 bo_free(struct iris_bo *bo)
 {
@@ -1525,12 +1541,19 @@ bo_free(struct iris_bo *bo)
    if (!bo->real.userptr && bo->real.map)
       bo_unmap(bo);
 
-   if (bo->idle || !iris_bo_busy(bo)) {
+   if (bo->idle || !iris_bo_busy(bo, 3)) {
       bo_close(bo);
    } else {
       /* Defer closing the GEM BO and returning the VMA for reuse until the
        * BO is idle.  Just move it to the dead list for now.
        */
+      zombie_total += bo->size;
+      printf("%s:%d add zombie: id: %u size: %lu total: %lu B/%lu MB\n",
+             __func__, __LINE__,bo->gem_handle, bo->size, zombie_total, zombie_total / (1<<20));
+      struct timespec time;
+
+      clock_gettime(CLOCK_MONOTONIC, &time);
+      bo->real.free_time = time.tv_sec;
       list_addtail(&bo->head, &bufmgr->zombie_list);
    }
 }
@@ -1565,9 +1588,14 @@ cleanup_bo_cache(struct iris_bufmgr *bufmgr, time_t time)
       /* Stop once we reach a busy BO - all others past this point were
        * freed more recently so are likely also busy.
        */
-      if (!bo->idle && iris_bo_busy(bo))
+      if (!bo->idle && iris_bo_busy(bo, 4))
+         break;
+
+      if (time - bo->real.free_time <= 5)
          break;
 
+zombie_total -= bo->size;
+printf("%s:%d REMOVE ZOMBIE id: %u\n", __func__, __LINE__, bo->gem_handle);
       list_del(&bo->head);
       bo_close(bo);
    }
diff --git a/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.h b/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.h
index 0500b89..04c7085 100644
--- a/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.h
+++ b/src/lib/mesa/src/gallium/drivers/iris/iris_bufmgr.h
@@ -508,7 +508,7 @@ void iris_bo_mark_exported(struct iris_bo *bo);
  * Returns true  if mapping the buffer for write could cause the process
  * to block, due to the object being active in the GPU.
  */
-bool iris_bo_busy(struct iris_bo *bo);
+bool iris_bo_busy(struct iris_bo *bo, int id);
 
 struct iris_bufmgr *iris_bufmgr_get_for_fd(int fd, bool bo_reuse);
 int iris_bufmgr_get_fd(struct iris_bufmgr *bufmgr);
diff --git a/src/lib/mesa/src/gallium/drivers/iris/iris_perf.c b/src/lib/mesa/src/gallium/drivers/iris/iris_perf.c
index 407030d..96e61eb 100644
--- a/src/lib/mesa/src/gallium/drivers/iris/iris_perf.c
+++ b/src/lib/mesa/src/gallium/drivers/iris/iris_perf.c
@@ -80,7 +80,7 @@ typedef void (*store_register_mem_t)(void *ctx, void *bo,
                                      uint32_t offset);
 typedef bool (*batch_references_t)(void *batch, void *bo);
 typedef void (*bo_wait_rendering_t)(void *bo);
-typedef int (*bo_busy_t)(void *bo);
+typedef int (*bo_busy_t)(void *bo, int id);
 
 void
 iris_perf_init_vtbl(struct intel_perf_config *perf_cfg)
diff --git a/src/lib/mesa/src/gallium/drivers/iris/iris_resource.c b/src/lib/mesa/src/gallium/drivers/iris/iris_resource.c
index 9b9bfd2..50eb6f0 100644
--- a/src/lib/mesa/src/gallium/drivers/iris/iris_resource.c
+++ b/src/lib/mesa/src/gallium/drivers/iris/iris_resource.c
@@ -1904,7 +1904,7 @@ static bool
 resource_is_busy(struct iris_context *ice,
                  struct iris_resource *res)
 {
-   bool busy = iris_bo_busy(res->bo);
+   bool busy = iris_bo_busy(res->bo, 5);
 
    iris_foreach_batch(ice, batch)
       busy |= iris_batch_references(batch, res->bo);
diff --git a/src/lib/mesa/src/intel/perf/intel_perf.h b/src/lib/mesa/src/intel/perf/intel_perf.h
index 33d1206..9164ee7 100644
--- a/src/lib/mesa/src/intel/perf/intel_perf.h
+++ b/src/lib/mesa/src/intel/perf/intel_perf.h
@@ -402,7 +402,7 @@ struct intel_perf_config {
       void (*bo_unmap)(void *bo);
       bool (*batch_references)(void *batch, void *bo);
       void (*bo_wait_rendering)(void *bo);
-      int (*bo_busy)(void *bo);
+      int (*bo_busy)(void *bo, int id);
       void (*emit_stall_at_pixel_scoreboard)(void *ctx);
       void (*emit_mi_report_perf_count)(void *ctx,
                                         void *bo,
diff --git a/src/lib/mesa/src/intel/perf/intel_perf_query.c b/src/lib/mesa/src/intel/perf/intel_perf_query.c
index 7de6867..eb4a841 100644
--- a/src/lib/mesa/src/intel/perf/intel_perf_query.c
+++ b/src/lib/mesa/src/intel/perf/intel_perf_query.c
@@ -1076,7 +1076,7 @@ read_oa_samples_for_query(struct intel_perf_context *perf_ctx,
    /* We need the MI_REPORT_PERF_COUNT to land before we can start
     * accumulate. */
    assert(!perf_cfg->vtbl.batch_references(current_batch, query->oa.bo) &&
-          !perf_cfg->vtbl.bo_busy(query->oa.bo));
+          !perf_cfg->vtbl.bo_busy(query->oa.bo, 7));
 
    /* Map the BO once here and let accumulate_oa_reports() unmap
     * it. */
@@ -1157,12 +1157,12 @@ intel_perf_is_query_ready(struct intel_perf_context *perf_ctx,
       return (query->oa.results_accumulated ||
               (query->oa.bo &&
                !perf_cfg->vtbl.batch_references(current_batch, query->oa.bo) &&
-               !perf_cfg->vtbl.bo_busy(query->oa.bo)));
+               !perf_cfg->vtbl.bo_busy(query->oa.bo, 8)));
 
    case INTEL_PERF_QUERY_TYPE_PIPELINE:
       return (query->pipeline_stats.bo &&
               !perf_cfg->vtbl.batch_references(current_batch, query->pipeline_stats.bo) &&
-              !perf_cfg->vtbl.bo_busy(query->pipeline_stats.bo));
+              !perf_cfg->vtbl.bo_busy(query->pipeline_stats.bo, 9));
 
    default:
       unreachable("Unknown query type");
